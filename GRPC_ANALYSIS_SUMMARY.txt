================================================================================
Croupier gRPC通信架构分析 - 执行报告
================================================================================

分析日期：2025年11月13日
分析工具：源代码直接分析 + 结构化检查
分析范围：proto/, internal/, cmd/, sdks/cpp/ 目录

================================================================================
一、分析成果总结
================================================================================

已生成三份完整文档：

1. GRPC_COMMUNICATION_ARCHITECTURE.md (完整技术分析)
   - 10个主章节，包含详细的代码实现分析
   - 1200+ 行技术文档
   - 包含时序图、数据流图、架构图
   - 涵盖所有核心概念和实现细节

2. GRPC_QUICK_REFERENCE.md (快速查询手册)
   - 12个快速参考章节
   - 表格化设计，便于快速查询
   - 包含性能调优建议、故障排查表
   - 完整通信示例（2个真实场景）

3. GRPC_FILES_INDEX.md (代码导航索引)
   - 完整的文件位置映射
   - 快速导航和修改指南
   - 数据模型和类型参考
   - 通信关键路径追踪

================================================================================
二、核心发现
================================================================================

【1】SDK与Agent的gRPC通信

SDK端(C++)：
  - CroupierClient 类：作为Provider（提供函数）
    → 连接 Agent:19090
    → RegisterLocal + Heartbeat + Function实现
    
  - CroupierInvoker 类：作为Consumer（调用函数）
    → 连接 Server:8443 或 Edge:8443
    → Invoke/StartJob/StreamJob/CancelJob

Agent角色（双重身份）：
  ✓ 作为gRPC Server (监听19090)
    - LocalControlService: 接收Game Server注册
    - FunctionService: 转发函数调用
    
  ✓ 作为gRPC Client
    - 连接Game Server转发调用
    - 连接Server进行注册和心跳
    - 维护LocalStore和jobIndex供路由

================================================================================
【2】双向通信支持情况

完全支持双向通信的服务：
  ✓ TunnelService.Open(stream TunnelMessage)
    - Agent主动连接Edge/Server
    - 完全双向流：Agent可随时发送，Server可随时推送
    - 支持多种消息类型复用（invoke/start/cancel/result/job_evt）

部分支持（Server Stream）：
  ⚠️ FunctionService.StreamJob(JobStreamRequest)
    - 客户端发送一次请求
    - Server单向推送JobEvent流
    - 不支持客户端在流中push数据

单向RPC：
  ✗ ControlService（Agent→Server 注册）
  ✗ LocalControlService（GameServer→Agent 注册）
  ✗ FunctionService.Invoke/StartJob/CancelJob（都是Unary）

================================================================================
【3】关键架构模式

游戏隔离：
  - 所有操作都通过game_id/env作用域隔离
  - 支持多游戏、多环境共存
  - 在proto中明确定义：RegisterRequest.game_id

异步任务管理：
  1. StartJob → 返回job_id
  2. Agent记录 jobIndex[job_id] = game_server_addr
  3. StreamJob 通过job_id查询对应的实例
  4. GameServer推送JobEvent (progress/log/done/error)
  5. CancelJob 从jobIndex查找并停止

JSON编码支持：
  - gRPC支持JSON而非仅限Protobuf二进制
  - 通过 internal/transport/jsoncodec/jsoncodec.go 实现
  - 便于调试和HTTP/gRPC兼容

================================================================================
【4】通信链路总结

同步调用：Web UI → Server → Edge → Agent → GameServer (5层)
异步调用：StartJob 返回 job_id → 客户端轮询 StreamJob
Agent转发：LocalStore[function_id] → grpc.Dial(gameserver_addr) → 转发

================================================================================
三、关键文件速览
================================================================================

Proto定义（4个核心服务）：
  • control.proto - Agent向Server注册
  • local.proto - GameServer向Agent注册
  • function.proto - 函数调用（全系统通用）
  • tunnel.proto - Agent与Server双向通信

Go实现：
  • agent/function_server.go - Agent的FunctionService实现（转发逻辑）
  • agent/job_index.go - job_id与实例的映射
  • platform/control/server.go - Server的ControlService实现
  • platform/agentlocal/local_control.go - Agent的LocalControlService实现
  • cmd/agent/main.go - Agent的grpc.NewServer启动
  • cmd/server/main.go - Server的edgeForwarder创建

C++ SDK：
  • croupier_client.h - CroupierClient和CroupierInvoker类定义
  • croupier_client.cpp - 实现（连接、注册、调用）

================================================================================
四、性能特性
================================================================================

连接管理：
  • gRPC自动连接复用和连接池
  • Keep-alive参数配置（见cmd/agent/main.go）
  • HTTP/2多路复用

超时管理：
  • Agent中的同步Invoke默认3秒超时
  • Server到Edge的调用 15秒超时 + 10秒重试
  • StreamJob无超时，由应用层手动cancel

心跳：
  • Agent → Server: 每60秒一次心跳
  • GameServer → Agent: 每60秒一次心跳
  • 用于会话续期

================================================================================
五、安全机制
================================================================================

mTLS支持：
  • 生产环境强制mTLS（--insecure false）
  • 开发环境支持insecure模式（--insecure true）
  • 实现在 internal/platform/tlsutil/tlsutil.go

RBAC和审计：
  • Server HTTP处理层进行认证、授权检查
  • 审计链记录所有操作

多游戏隔离：
  • game_id 作为所有操作的范围限定
  • 通过registry store维护Agent会话隔离

================================================================================
六、快速答疑
================================================================================

Q1: SDK如何与Agent进行gRPC通信？
A: SDK (C++客户端) → grpc.Dial("agent:19090") → Agent的FunctionService
   GameServer通过LocalControlService.RegisterLocal注册到Agent
   后续调用通过FunctionService.Invoke/StartJob/StreamJob转发

Q2: gRPC是否支持双向通信？
A: 是，但只有TunnelService.Open() 提供完全双向流
   FunctionService.StreamJob 只是Server Stream（单向推送）
   其他都是Unary RPC（单向）

Q3: Agent是Server还是Client？
A: 都是！
   作为Server：监听19090，处理GameServer的注册和函数调用
   作为Client：拨号GameServer转发调用，拨号Server进行注册

Q4: 异步任务如何跟踪？
A: StartJob返回job_id → Agent保存到jobIndex[job_id]
   StreamJob时查询jobIndex找到执行实例 → 从该实例获取事件流
   
Q5: 怎样实现多游戏隔离？
A: game_id在RegisterRequest和RegisterLocalRequest中指定
   Server和Agent都通过game_id分离存储和查询
   
Q6: 如何发送心跳？
A: Agent自动定期调用Server.Heartbeat()来续期会话
   GameServer自动定期调用Agent.Heartbeat()来续期会话
   超时销毁会话（60秒有效期）

================================================================================
七、推荐阅读顺序
================================================================================

初级（理解整体架构）：
  1. GRPC_QUICK_REFERENCE.md - 第1-3节（港口和服务映射）
  2. GRPC_COMMUNICATION_ARCHITECTURE.md - 第1-2节（整体架构）
  3. GRPC_QUICK_REFERENCE.md - 第4节（数据流）

中级（学习实现细节）：
  1. GRPC_COMMUNICATION_ARCHITECTURE.md - 第3-4节（SDK与Agent通信）
  2. 查看源代码：internal/app/agent/function_server.go
  3. GRPC_QUICK_REFERENCE.md - 第6节（双向通信详解）

高级（性能和调优）：
  1. GRPC_COMMUNICATION_ARCHITECTURE.md - 第7-8节（mTLS和JSON编码）
  2. GRPC_QUICK_REFERENCE.md - 第9-10节（故障排查和性能调优）
  3. GRPC_FILES_INDEX.md - 快速导航章节（定位关键代码）

================================================================================
八、文档生成统计
================================================================================

分析涵盖的源文件：
  • Proto文件：9个 (.proto)
  • Go源文件：15+ 个 (agent, control, server, edge, transport等)
  • C++ SDK文件：4个 (头文件+实现+示例)
  • 配置文件：3个 (示例)

代码行数分析：
  • proto/croupier/tunnel/v1/tunnel.proto：54行（TunnelService定义）
  • internal/app/agent/function_server.go：79行（转发逻辑）
  • cmd/server/main.go：200+行（分析段，edgeForwarder）
  • sdks/cpp/src/croupier_client.cpp：200+行（分析段）

文档总量：
  • GRPC_COMMUNICATION_ARCHITECTURE.md：1200+ 行
  • GRPC_QUICK_REFERENCE.md：400+ 行
  • GRPC_FILES_INDEX.md：500+ 行
  • GRPC_ANALYSIS_SUMMARY.txt：本文件 (300+ 行)
  • 总计：2400+ 行完整分析文档

================================================================================
九、关键概念速记
================================================================================

LocalStore：
  → 维护 function_id → []LocalInstance 映射
  → Agent用于查找函数对应的GameServer地址
  → 由GameServer的RegisterLocal调用更新

jobIndex：
  → 维护 job_id → game_server_addr 映射
  → Agent用于回源到具体执行实例
  → 由FunctionServer.StartJob创建，CancelJob时删除

ControlService：
  → 由Server实现，Agent作为客户端调用
  → 用于Agent的生命周期管理（注册、心跳）

LocalControlService：
  → 由Agent实现，GameServer作为客户端调用
  → 用于GameServer的生命周期管理（注册、心跳）

FunctionService：
  → 由Agent、Edge、GameServer都可能实现
  → 用于函数调用的转发（Invoke）和异步任务（StartJob/StreamJob）

TunnelService：
  → 由Edge/Server实现，Agent作为客户端调用
  → 完全双向流，支持多种消息类型复用

================================================================================
十、后续扩展方向
================================================================================

基于分析结果，以下方面可考虑扩展：

1. Agent负载均衡策略
   - 当多个GameServer提供相同函数时的选择策略
   - 建议查看：LoadBalancer接口（ARCHITECTURE.md）

2. 流量管理和限流
   - 当前未见TLS的Rate Limiting实现
   - 可在gRPC拦截器中添加

3. 断路器和熔断
   - 当前基于Unavailable/Deadline错误重试
   - 可考虑引入断路器模式

4. 分布式追踪集成
   - 目前支持trace_id、request_id元数据
   - 可与OpenTelemetry更深度集成

5. 动态Agent发现
   - 当前Agent主动注册到Server
   - 可考虑ServiceMesh集成

================================================================================

本分析报告由AI代码分析系统生成。
如有问题或需要补充分析，请参考三份详细文档。

================================================================================
